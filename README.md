# ETL_PIPELINE

# ETL Pipeline Project  
**Titanic Dataset & Telco Customer Churn Dataset**

## ğŸ“Œ Project Overview
This project demonstrates a complete **ETL (Extract, Transform, Load) pipeline** built using **Python, Pandas, and Supabase (PostgreSQL)**.  
The pipeline processes two real-world datasets:

- ğŸš¢ **Titanic Dataset** â€“ Passenger survival data
- ğŸ“¡ **WA_Fn-UseC_-Telco-Customer-Churn Dataset** â€“ Customer churn analytics

The goal is to perform data extraction, cleaning, feature engineering, validation, loading into a cloud database, and basic analysis.

---

## ğŸ§© Datasets Used

### 1ï¸âƒ£ Titanic Dataset
- Passenger demographics
- Ticket, fare, cabin details
- Survival label

### 2ï¸âƒ£ Telco Customer Churn Dataset
- Customer demographics
- Service subscriptions
- Contract and payment details
- Churn status

---

## ğŸ—ï¸ ETL Pipeline Architecture

Raw Data
â†“
Extract (CSV)
â†“
Transform (Cleaning + Feature Engineering)
â†“
Validate (Data Quality Checks)
â†“
Load (Supabase PostgreSQL)
â†“
Analyze (Metrics & Summary Reports)

## âš™ï¸ Technologies Used
- **Python 3**
- **Pandas**
- **Supabase (PostgreSQL)**
- **dotenv**
- **Git & GitHub**

  ## ğŸ“‚ Project Structure

  ETL_PIPELINE/
â”‚
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ raw/ # Original datasets
â”‚ â”œâ”€â”€ staged/ # Transformed datasets
â”‚ â””â”€â”€ processed/ # Analysis outputs
â”‚
â”œâ”€â”€ scripts/
â”‚ â”œâ”€â”€ extract.py # Data extraction
â”‚ â”œâ”€â”€ transform.py # Cleaning & feature engineering
â”‚ â”œâ”€â”€ load.py # Load data into Supabase
â”‚ â”œâ”€â”€ etl_analysis.py # Analysis & metrics
â”‚ â””â”€â”€ etl_validation.py # Data validation checks
â”‚
â”œâ”€â”€ .env # Supabase credentials
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md


---

## ğŸ”„ ETL Steps Explained

### âœ… Extract
- Load raw CSV files using Pandas.

### âœ… Transform
- Handle missing values.
- Convert data types.
- Feature engineering:
  - `tenure_group`
  - `monthly_charge_segment`
  - `has_internet_service`
  - `contract_type_code`
- Encode categorical variables.

### âœ… Validate
- No missing values in critical numeric fields.
- Row count consistency.
- Valid category segments.
- Contract codes limited to `{0,1,2}`.

### âœ… Load
- Load transformed data into **Supabase PostgreSQL**.
- Batch inserts with retry logic.
- NaN â†’ NULL handling.

### âœ… Analyze
- Churn percentage
- Average monthly charges per contract
- Tenure group distribution
- Internet service distribution
- Churn vs Tenure Group pivot
- Output saved as CSV

---

## ğŸ“Š Sample Analysis Metrics
- Overall churn rate
- Churn by tenure group
- Contract-wise monthly charges
- Customer segmentation insights

---

## ğŸ—„ï¸ Database
- **Supabase PostgreSQL**
- Cloud-hosted
- Table: `telco_customer_churn_features`

---

## ğŸš€ How to Run

1. Clone the repository
   ```bash
   git clone <repo-url>

2. Install dependencies

pip install -r requirements.txt


3. Configure .env

SUPABASE_URL=your_url
SUPABASE_KEY=your_key


4. Run ETL

python scripts/extract.py
python scripts/transform.py
python scripts/load.py
python scripts/etl_analysis.py
